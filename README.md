# Modality-Aware Transformer for Financial Time Series and Text

This repository contains an implementation and empirical study of a Modality-Aware Transformer (MAT) applied to financial prediction tasks using multimodal data, combining numerical time series (e.g. stock returns) and textual information (e.g. financial news).

The project is conducted in the context of a research-oriented machine learning study for the **ENSAE course Advanced Machine Learning_** given by M. Stromme.

**Authors**  
- Ramesh Brian  
- Sicard Audric  

---

## References

This project follows the experimental protocol of the original Modality-aware Transformer for Financial Time series Forecasting by Emami and al. (2023), paper, with comparisons against strong Transformer-based baselines.
> Available at: https://arxiv.org/abs/2310.01232

---

## Installation

```bash
to fill 
```
---
## Project Overview

Financial markets are influenced by both historical numerical signals and unstructured textual information. Classical Transformers struggle to efficiently fuse heterogeneous modalities.

The Modality-Aware Transformer (MAT) addresses this limitation by:
	•	Encoding each modality separately
	•	Using learned modality-aware queries
	•	Performing cross-modal attention in a structured manner

This project:
	•	Implements a clean and modular version of MAT in PyTorch
	•	Compares MAT against standard Transformer baselines
	•	Evaluates performance on financial return prediction tasks

---

## Repository Structure

---

## Usage to reproduce results
