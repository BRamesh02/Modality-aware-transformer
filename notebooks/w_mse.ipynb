{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a719abce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAM: 1.02 GB / 179.37 GB used\n",
      "GPU: NVIDIA A100-SXM4-80GB\n",
      "VRAM: 0.00 GB allocated\n"
     ]
    }
   ],
   "source": [
    "# Check RAM\n",
    "import psutil\n",
    "\n",
    "ram_gb = psutil.virtual_memory().total / 1e9\n",
    "used_gb = psutil.virtual_memory().used / 1e9\n",
    "print(f\"RAM: {used_gb:.2f} GB / {ram_gb:.2f} GB used\")\n",
    "\n",
    "# Check GPU\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB allocated\")\n",
    "else:\n",
    "    print(\"No GPU detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdef8c8",
   "metadata": {},
   "source": [
    "\n",
    "# MAT Training - Fold 1 (Weighted MSE)\n",
    "# * **Path:** Drive/MAT_Weighted_Loss/Modality-aware-transformer\n",
    "# * **Split:** Train (2010-2015), Val (2016), Test (2017)\n",
    "# * **Loss:** Weighted MSE (Alpha=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42f0722a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from pathlib import Path\n",
    "from google.colab import drive\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "319021df",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3604256599.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# --- 1. SETUP ENV ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Mount Google Drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"content/drive/MyDrive\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Define the exact path to your project\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    132\u001b[0m   )\n\u001b[1;32m    133\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# --- 1. SETUP ENV ---\n",
    "# Mount Google Drive\n",
    "drive.mount(\"content/drive/MyDrive\")\n",
    "\n",
    "# Define the exact path to your project\n",
    "# (Updated to your new folder structure)\n",
    "PROJECT_ROOT = \"/content/drive/MyDrive/MAT_Weighted_Loss/Modality-aware-transformer\"\n",
    "\n",
    "# Add this path to Python's list of folders to search for imports\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.append(PROJECT_ROOT)\n",
    "\n",
    "print(f\"Project Root set to: {PROJECT_ROOT}\")\n",
    "\n",
    "# Check if the path actually exists\n",
    "if not Path(PROJECT_ROOT).exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Could not find project at {PROJECT_ROOT}. Please check the folder names in Drive.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228bb100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. IMPORTS FROM SRC ---\n",
    "try:\n",
    "    from src.utils.data_loader import load_and_merge_data, prepare_scaled_fold\n",
    "    from src.models.dataset import FinancialDataset\n",
    "    from src.models.architectures.mat import MAT\n",
    "    from src.training.callbacks import EarlyStopping\n",
    "    from src.training.engine import train_epoch, validate_epoch\n",
    "    from src.evaluation.predictions.inference import WalkForwardEvaluator\n",
    "    print(\"SUCCESS: Local modules imported.\")\n",
    "except ImportError as e:\n",
    "    print(f\"ERROR: Could not import modules. {e}\")\n",
    "    print(\"Double check that 'src' folder exists inside the PROJECT_ROOT.\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using Device: {device} ({torch.cuda.get_device_name(0)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a71113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 3. Define Custom Loss\n",
    "# We define this here to ensure it's available immediately.\n",
    "\n",
    "class WeightedMSE(nn.Module):\n",
    "    \"\"\"\n",
    "    Weighted Mean Squared Error.\n",
    "    Penalizes errors on large targets more than errors on small targets.\n",
    "    Formula: Loss = (pred - target)^2 * (1 + alpha * |target|)\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=100.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def forward(self, preds, targets):\n",
    "        se = (preds - targets) ** 2\n",
    "        # Weighting: 1.0 for zero targets, increasing for larger targets\n",
    "        weights = 1 + self.alpha * torch.abs(targets)\n",
    "        return torch.mean(se * weights)\n",
    "\n",
    "# %% [markdown]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b6b452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 4. Data Loading & Preparation\n",
    "\n",
    "DATA_DIR = Path(PROJECT_ROOT) / \"data\"\n",
    "\n",
    "# 1. Load Data\n",
    "print(f\"Loading data from {DATA_DIR}...\")\n",
    "df_main = load_and_merge_data(DATA_DIR, start_date=\"2010-01-01\", end_date=\"2023-12-31\")\n",
    "\n",
    "# 2. Define Split for Fold 1\n",
    "split = {\n",
    "    'train': (\"2010-01-01\", \"2015-12-31\"),\n",
    "    'val':   (\"2016-01-01\", \"2016-12-31\"),\n",
    "    'test':  (\"2017-01-01\", \"2017-12-31\")\n",
    "}\n",
    "\n",
    "# 3. Features Setup\n",
    "non_feature_cols = [\n",
    "    \"date\", \"permno\", \"target\", \"emb_mean\", \n",
    "    \"sent_score_mean\", \"sent_pos_mean\", \"sent_neg_mean\", \n",
    "    \"log_n_news\", \"sent_score_std\"\n",
    "]\n",
    "\n",
    "# All input features (including has_news)\n",
    "all_num_cols = [c for c in df_main.columns if c not in non_feature_cols]\n",
    "\n",
    "# Features to actually scale (exclude binary flags)\n",
    "scale_cols = [c for c in all_num_cols if c != \"has_news\"]\n",
    "\n",
    "print(f\"Total Input Features: {len(all_num_cols)}\")\n",
    "print(f\"Features being scaled: {len(scale_cols)}\")\n",
    "\n",
    "# 4. Scale\n",
    "print(\"Scaling and splitting data...\")\n",
    "df_train, df_val, df_test = prepare_scaled_fold(df_main, scale_cols, split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8154dc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 5. Create Datasets & Loaders (A100 OPTIMIZED)\n",
    "\n",
    "# %%\n",
    "# Configuration for High-RAM + A100\n",
    "WINDOW_SIZE = 60\n",
    "FORECAST_HORIZON = 1\n",
    "\n",
    "# A100 Beast Mode Settings\n",
    "BATCH_SIZE = 4096   # Massive batch size for speed & stability\n",
    "NUM_WORKERS = 8     # Use more CPU cores to load data\n",
    "\n",
    "print(\"Creating PyTorch Datasets...\")\n",
    "train_ds = FinancialDataset(df_train, window_size=WINDOW_SIZE, forecast_horizon=FORECAST_HORIZON)\n",
    "val_ds   = FinancialDataset(df_val,   window_size=WINDOW_SIZE, forecast_horizon=FORECAST_HORIZON)\n",
    "test_ds  = FinancialDataset(df_test,  window_size=WINDOW_SIZE, forecast_horizon=FORECAST_HORIZON)\n",
    "\n",
    "# Clean RAM (even though you have plenty, it's good practice)\n",
    "del df_train, df_val, df_test\n",
    "gc.collect()\n",
    "\n",
    "print(f\"Train Size: {len(train_ds)}\")\n",
    "print(f\"Val Size:   {len(val_ds)}\")\n",
    "print(f\"Test Size:  {len(test_ds)}\")\n",
    "\n",
    "# Loaders\n",
    "# persistent_workers=True keeps the workers alive between epochs (saves startup time)\n",
    "train_loader = DataLoader(\n",
    "    train_ds, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    num_workers=NUM_WORKERS, \n",
    "    pin_memory=True,\n",
    "    persistent_workers=True \n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_ds,   \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    num_workers=NUM_WORKERS, \n",
    "    pin_memory=True,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_ds,  \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    num_workers=NUM_WORKERS, \n",
    "    pin_memory=True,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "print(f\"Data Loaders Ready! Batch Size: {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b9f1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 6. Initialize Model & Training\n",
    "\n",
    "# Input dim must match ALL columns (22 approx)\n",
    "model = MAT(\n",
    "    num_input_dim=len(all_num_cols), \n",
    "    n_sent=5, \n",
    "    d_model=128,\n",
    "    forecast_horizon=FORECAST_HORIZON\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "# --- USING WEIGHTED MSE ---\n",
    "criterion = WeightedMSE(alpha=100.0) \n",
    "\n",
    "# Save Checkpoints to Drive so you don't lose them if Colab disconnects\n",
    "ckpt_dir = Path(PROJECT_ROOT) / \"models\"\n",
    "ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "ckpt_path = ckpt_dir / \"mat_weighted_fold1.pt\"\n",
    "\n",
    "early_stopping = EarlyStopping(patience=5, path=str(ckpt_path), verbose=True)\n",
    "\n",
    "print(\"Starting Training...\")\n",
    "EPOCHS = 20\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    t_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    v_loss = validate_epoch(model, val_loader, criterion, device)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} | Train Loss: {t_loss:.6f} | Val Loss: {v_loss:.6f}\")\n",
    "    \n",
    "    early_stopping(v_loss, model)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early Stopping Triggered!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4d537e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 7. Inference & Variance Check\n",
    "\n",
    "print(\"Loading Best Model for Inference...\")\n",
    "model.load_state_dict(torch.load(ckpt_path))\n",
    "\n",
    "evaluator = WalkForwardEvaluator(model, device)\n",
    "print(\"Running Inference on Test Set (2017)...\")\n",
    "df_preds = evaluator.predict_fold(test_loader, fold_name=\"MAT_Weighted_2017\")\n",
    "\n",
    "# --- Stats Check ---\n",
    "print(\"\\n--- Variance Analysis ---\")\n",
    "pred_std = df_preds['pred'].std()\n",
    "target_std = df_preds['target'].std()\n",
    "ratio = pred_std / target_std\n",
    "\n",
    "print(f\"Pred Std Dev:   {pred_std:.6f}\")\n",
    "print(f\"Target Std Dev: {target_std:.6f}\")\n",
    "print(f\"Ratio: {ratio:.4f}\")\n",
    "\n",
    "if ratio < 0.05:\n",
    "    print(\"⚠️ WARNING: Predictions still look collapsed (Low Variance).\")\n",
    "else:\n",
    "    print(\"✅ SUCCESS: Model variance looks healthy.\")\n",
    "\n",
    "# Save Predictions to Drive\n",
    "save_path = Path(PROJECT_ROOT) / \"data\" / \"processed\" / \"predictions\" / \"mat_weighted_2017.csv\"\n",
    "save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "df_preds.to_csv(save_path, index=False)\n",
    "print(f\"Predictions saved to {save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
